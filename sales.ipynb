{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science toolkit and application\n",
    "# Case study 1 : Explore data analysis fundamentals\n",
    "## Basic data manipulation and visualization\n",
    " \n",
    "\n",
    "By this case study, you will be able to:\n",
    "- Set up Python environment and VS Code, using environment management tools like virtualenv or conda\n",
    "- Load and explore a dataset using Python basics and Pandas, including basic DataFrame operations\n",
    "- Implement error handling when loading and exploring a dataset\n",
    "- Perform simple aggregations on a dataset\n",
    "- Create basic visualizations with Matplotlib\n",
    "- Implement simple linear regression for prediction\n",
    "- Use Git for version control\n",
    "    \n",
    "**Documentation references:**\n",
    "- [Numpy for beginners](https://numpy.org/devdocs/user/absolute_beginners.html)  \n",
    "- [Getting started with pandas](https://pandas.pydata.org/docs/getting_started/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "# 1. Pandas package discovery\n",
    "**Pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way toward this goal.\n",
    "\n",
    "\n",
    "Pandas is well suited for many different kinds of data:\n",
    "* Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "* Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "* Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
    "* Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure\n",
    "\n",
    "\n",
    "The two primary data structures of pandas, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.html#pandas.Series\" target=\"_blank\">Series</a>  (1-dimensional) and <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame\" target=\"_blank\">DataFrame</a>  (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering.\n",
    "Pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Getting started with Pandas \n",
    "\n",
    "To load the pandas package and start working with it, you need to import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.\n",
    "\n",
    "This can be easily done with this import statement in Python. \n",
    "Learn how to do it <a href=\"https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html\" target=\"_blank\">here</a>, then fill the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas package\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"        \" target=\"_blank\">        </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What kind of data does pandas handle?\n",
    "\n",
    "Pandas can handle **dataframes**\n",
    "A Pandas DataFrame is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns.\n",
    "A DataFrame is thus a dat astructure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns.\n",
    "\n",
    "You can see more details about Pandas dataframe structure, parameters, and functions in the  <a href=\" https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html \" target=\"_blank\">       documentation</a>.\n",
    "\n",
    "<img src=\"images/pandas_table_dataframe.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and read a tabular data\n",
    "Pandas provides the `read_*()` function to read data stored as a `*` file into a pandas DataFrame. Pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, …), each of them with the prefix `read_*`.\n",
    "\n",
    "![](images/pandas_read_table.png)\n",
    "\n",
    "Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default.\n",
    "\n",
    "Now, we would like to load and read the sales dataset. This dataset contains information about the purchase information (product, quantity, price, address...).\n",
    "In the code cell below, read the `Sales_Analysis.csv` dataset, display first and last rows.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "- [Data loading best practices](https://pandas.pydata.org/docs/user_guide/io.html)\n",
    "- [File path handling in pandas](https://pandas.pydata.org/docs/user_guide/io.html#reading-files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Load and examine sales transaction dataset\n",
    "\n",
    "Sales transaction data provides insights into customer behavior, product performance, and revenue patterns. Proper data loading establishes the foundation for all subsequent analysis by ensuring data integrity and understanding the dataset structure.\n",
    "\n",
    "Load the sales dataset. This creates a DataFrame object that allows efficient manipulation and analysis of tabular data. Always verify successful loading by examining the dataset structure immediately after import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Read the Sales_Analysis.csv file\n",
    "data = pd.read_csv('sales.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/19/19 08:46</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/07/19 22:30</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>259353</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>3</td>\n",
       "      <td>2.99</td>\n",
       "      <td>09/17/19 20:56</td>\n",
       "      <td>840 Highland St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>259354</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/01/19 16:00</td>\n",
       "      <td>216 Dogwood St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>259355</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/23/19 07:39</td>\n",
       "      <td>220 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>259356</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>09/19/19 17:30</td>\n",
       "      <td>511 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>259357</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>09/30/19 00:18</td>\n",
       "      <td>250 Meadow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186850 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0        176558        USB-C Charging Cable                2      11.95   \n",
       "1           NaN                         NaN              NaN        NaN   \n",
       "2        176559  Bose SoundSport Headphones                1      99.99   \n",
       "3        176560                Google Phone                1        600   \n",
       "4        176560            Wired Headphones                1      11.99   \n",
       "...         ...                         ...              ...        ...   \n",
       "186845   259353      AAA Batteries (4-pack)                3       2.99   \n",
       "186846   259354                      iPhone                1        700   \n",
       "186847   259355                      iPhone                1        700   \n",
       "186848   259356      34in Ultrawide Monitor                1     379.99   \n",
       "186849   259357        USB-C Charging Cable                1      11.95   \n",
       "\n",
       "            Order Date                         Purchase Address  \n",
       "0       04/19/19 08:46             917 1st St, Dallas, TX 75001  \n",
       "1                  NaN                                      NaN  \n",
       "2       04/07/19 22:30        682 Chestnut St, Boston, MA 02215  \n",
       "3       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "4       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "...                ...                                      ...  \n",
       "186845  09/17/19 20:56   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  09/01/19 16:00  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847  09/23/19 07:39     220 12th St, San Francisco, CA 94016  \n",
       "186848  09/19/19 17:30   511 Forest St, San Francisco, CA 94016  \n",
       "186849  09/30/19 00:18   250 Meadow St, San Francisco, CA 94016  \n",
       "\n",
       "[186850 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the pandas dataframe \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/19/19 08:46</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/07/19 22:30</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>176561</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/30/19 09:27</td>\n",
       "      <td>333 8th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>176562</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/29/19 13:03</td>\n",
       "      <td>381 Wilson St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>176563</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/02/19 07:46</td>\n",
       "      <td>668 Center St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0   176558        USB-C Charging Cable                2      11.95   \n",
       "1      NaN                         NaN              NaN        NaN   \n",
       "2   176559  Bose SoundSport Headphones                1      99.99   \n",
       "3   176560                Google Phone                1        600   \n",
       "4   176560            Wired Headphones                1      11.99   \n",
       "5   176561            Wired Headphones                1      11.99   \n",
       "6   176562        USB-C Charging Cable                1      11.95   \n",
       "7   176563  Bose SoundSport Headphones                1      99.99   \n",
       "\n",
       "       Order Date                        Purchase Address  \n",
       "0  04/19/19 08:46            917 1st St, Dallas, TX 75001  \n",
       "1             NaN                                     NaN  \n",
       "2  04/07/19 22:30       682 Chestnut St, Boston, MA 02215  \n",
       "3  04/12/19 14:38    669 Spruce St, Los Angeles, CA 90001  \n",
       "4  04/12/19 14:38    669 Spruce St, Los Angeles, CA 90001  \n",
       "5  04/30/19 09:27       333 8th St, Los Angeles, CA 90001  \n",
       "6  04/29/19 13:03  381 Wilson St, San Francisco, CA 94016  \n",
       "7  04/02/19 07:46        668 Center St, Seattle, WA 98101  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Display the first 8 rows of a pandas DataFrame.\n",
    "data.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>259353</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>3</td>\n",
       "      <td>2.99</td>\n",
       "      <td>09/17/19 20:56</td>\n",
       "      <td>840 Highland St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>259354</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/01/19 16:00</td>\n",
       "      <td>216 Dogwood St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>259355</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/23/19 07:39</td>\n",
       "      <td>220 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>259356</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>09/19/19 17:30</td>\n",
       "      <td>511 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>259357</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>09/30/19 00:18</td>\n",
       "      <td>250 Meadow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                 Product Quantity Ordered Price Each  \\\n",
       "186845   259353  AAA Batteries (4-pack)                3       2.99   \n",
       "186846   259354                  iPhone                1        700   \n",
       "186847   259355                  iPhone                1        700   \n",
       "186848   259356  34in Ultrawide Monitor                1     379.99   \n",
       "186849   259357    USB-C Charging Cable                1      11.95   \n",
       "\n",
       "            Order Date                         Purchase Address  \n",
       "186845  09/17/19 20:56   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  09/01/19 16:00  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847  09/23/19 07:39     220 12th St, San Francisco, CA 94016  \n",
       "186848  09/19/19 17:30   511 Forest St, San Francisco, CA 94016  \n",
       "186849  09/30/19 00:18   250 Meadow St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Show the last 5 rows in the dataset with the tail() method.\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Structure Analysis\n",
    "\n",
    "Understanding dataset properties is crucial before any analysis. Sales data often contains mixed data types due to inconsistent data entry, requiring careful examination to identify appropriate preprocessing steps. Proper data type identification ensures mathematical operations work correctly and prevents analysis errors.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.shape](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html)\n",
    "- [pandas.DataFrame.dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html)\n",
    "- [pandas.DataFrame.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)\n",
    "- [Understanding pandas data types](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Examine dataset dimensions and structure\n",
    "\n",
    "Dataset dimensionality reveals the scale of your analysis challenge. Understanding how many transactions (rows) and variables (columns) you're working with helps estimate processing requirements and identifies potential sampling needs for large datasets.\n",
    "\n",
    "Check the dataset dimensions using `.shape` to understand the volume of data available for analysis. This tuple shows (number_of_rows, number_of_columns) and helps assess whether you have sufficient data for meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186850, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check the number of rows and columns in the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze column data types\n",
    "\n",
    "Data type identification is critical for sales analysis because columns may appear numeric but actually contain text, preventing mathematical operations. The `dtypes` property reveals how pandas interprets each column, while mixed-type columns default to 'object' type.\n",
    "\n",
    "Examine column data types using `.dtypes` to identify which columns contain numeric data (for calculations), text data (for categorization), or mixed types (requiring cleaning). Pay attention to columns that should be numeric but appear as 'object' type - these often contain data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            object\n",
       "Product             object\n",
       "Quantity Ordered    object\n",
       "Price Each          object\n",
       "Order Date          object\n",
       "Purchase Address    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check the datatypes of the dataset\n",
    "data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Generate comprehensive dataset summary\n",
    "\n",
    "The `.info()` method provides a complete overview including data types, missing values, and memory usage. This summary helps identify data quality issues and planning cleaning strategies before analysis begins.\n",
    "Use `.info()` to get detailed information about each column including non-null counts (revealing missing data), data types, and memory usage. This comprehensive view helps prioritize which columns need cleaning or type conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0        176558        USB-C Charging Cable                2      11.95   \n",
       "1           NaN                         NaN              NaN        NaN   \n",
       "2        176559  Bose SoundSport Headphones                1      99.99   \n",
       "3        176560                Google Phone                1        600   \n",
       "4        176560            Wired Headphones                1      11.99   \n",
       "...         ...                         ...              ...        ...   \n",
       "186845   259353      AAA Batteries (4-pack)                3       2.99   \n",
       "186846   259354                      iPhone                1        700   \n",
       "186847   259355                      iPhone                1        700   \n",
       "186848   259356      34in Ultrawide Monitor                1     379.99   \n",
       "186849   259357        USB-C Charging Cable                1      11.95   \n",
       "\n",
       "            Order Date                         Purchase Address  \n",
       "0       04/19/19 08:46             917 1st St, Dallas, TX 75001  \n",
       "1                  NaN                                      NaN  \n",
       "2       04/07/19 22:30        682 Chestnut St, Boston, MA 02215  \n",
       "3       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "4       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "...                ...                                      ...  \n",
       "186845  09/17/19 20:56   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  09/01/19 16:00  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847  09/23/19 07:39     220 12th St, San Francisco, CA 94016  \n",
       "186848  09/19/19 17:30   511 Forest St, San Francisco, CA 94016  \n",
       "186849  09/30/19 00:18   250 Meadow St, San Francisco, CA 94016  \n",
       "\n",
       "[186850 rows x 6 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Print a concise summary of a DataFrame\n",
    "data.info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When printing precise information, please notice the following items:**\n",
    "-  `<class 'pandas.core.frame.DataFrame'>`: Indicates that this is a pandas DataFrame.\n",
    "- `RangeIndex: Shows the index range of the DataFrame.\n",
    "- `Data columns` (here, total 6 columns): Indicates that there are 6 columns in the DataFrame.\n",
    "- `dtypes`: Displays the data types of each column.\n",
    "- `memory usage`: Represents the memory usage of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Cleaning and Preprocessing\n",
    "\n",
    "Real-world sales data frequently contains missing values and duplicate records that can skew analysis results. Missing data may occur due to system errors, incomplete customer information, or data transmission issues. Duplicate transactions can artificially inflate sales metrics and lead to incorrect business insights. Systematic data cleaning ensures analysis accuracy and reliability.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html)\n",
    "- [pandas.DataFrame.dropna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)\n",
    "- [pandas.DataFrame.duplicated()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)\n",
    "- [pandas.DataFrame.drop_duplicates()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)\n",
    "- [Missing data handling guide](https://pandas.pydata.org/docs/user_guide/missing_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Comprehensive data quality assessment and cleaning\n",
    "\n",
    "Data quality directly impacts analysis reliability. Missing values can bias statistical calculations, while duplicate records inflate metrics and distort business insights. A systematic cleaning approach identifies these issues and removes problematic records while preserving data integrity.\n",
    "\n",
    "Perform a complete data quality assessment by first counting missing values in each column. Remove rows containing missing values to ensure complete records for analysis. \n",
    "\n",
    "Next, identify duplicate transactions, then remove them. Finally, verify the cleaning effectiveness by rechecking missing values and duplicates, and examine the new dataset dimensions using to understand how much data was removed during the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            545\n",
       "Product             545\n",
       "Quantity Ordered    545\n",
       "Price Each          545\n",
       "Order Date          545\n",
       "Purchase Address    545\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Count the number of Null values for each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/19/19 08:46</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/07/19 22:30</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>176561</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/30/19 09:27</td>\n",
       "      <td>333 8th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>259353</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>3</td>\n",
       "      <td>2.99</td>\n",
       "      <td>09/17/19 20:56</td>\n",
       "      <td>840 Highland St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>259354</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/01/19 16:00</td>\n",
       "      <td>216 Dogwood St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>259355</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/23/19 07:39</td>\n",
       "      <td>220 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>259356</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>09/19/19 17:30</td>\n",
       "      <td>511 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>259357</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>09/30/19 00:18</td>\n",
       "      <td>250 Meadow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186305 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0        176558        USB-C Charging Cable                2      11.95   \n",
       "2        176559  Bose SoundSport Headphones                1      99.99   \n",
       "3        176560                Google Phone                1        600   \n",
       "4        176560            Wired Headphones                1      11.99   \n",
       "5        176561            Wired Headphones                1      11.99   \n",
       "...         ...                         ...              ...        ...   \n",
       "186845   259353      AAA Batteries (4-pack)                3       2.99   \n",
       "186846   259354                      iPhone                1        700   \n",
       "186847   259355                      iPhone                1        700   \n",
       "186848   259356      34in Ultrawide Monitor                1     379.99   \n",
       "186849   259357        USB-C Charging Cable                1      11.95   \n",
       "\n",
       "            Order Date                         Purchase Address  \n",
       "0       04/19/19 08:46             917 1st St, Dallas, TX 75001  \n",
       "2       04/07/19 22:30        682 Chestnut St, Boston, MA 02215  \n",
       "3       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "4       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "5       04/30/19 09:27        333 8th St, Los Angeles, CA 90001  \n",
       "...                ...                                      ...  \n",
       "186845  09/17/19 20:56   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  09/01/19 16:00  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847  09/23/19 07:39     220 12th St, San Francisco, CA 94016  \n",
       "186848  09/19/19 17:30   511 Forest St, San Francisco, CA 94016  \n",
       "186849  09/30/19 00:18   250 Meadow St, San Francisco, CA 94016  \n",
       "\n",
       "[186305 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Remove null values from the dataset\n",
    "df_no_null = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check if missing rows have been removed\n",
    "df_no_null.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(618)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check for the duplicate values in the dataset\n",
    "df_no_null.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Drop duplicates\n",
    "df_no_null_dup = df_no_null.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check again\n",
    "df_no_null_dup.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185687, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Check the new dimensions of the dataframe\n",
    "df_no_null_dup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Focus on Data Cleaning and Preparation\n",
    "\n",
    "Let us make a small focus on a specific row, number 519, let us show its content.\n",
    "We can notice that the values of the columns seem to be wrong. This kind of wrong data is difficult to detect, as it is impossible to check each row in the dataframe manually. \n",
    "One of the main steps to avoid this kind of data, is to convert colums types to the right type that is consistent related to our understanding of the data. Let us do that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/19/19 08:46</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/07/19 22:30</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>176561</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/30/19 09:27</td>\n",
       "      <td>333 8th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>259353</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>3</td>\n",
       "      <td>2.99</td>\n",
       "      <td>09/17/19 20:56</td>\n",
       "      <td>840 Highland St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>259354</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/01/19 16:00</td>\n",
       "      <td>216 Dogwood St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>259355</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>09/23/19 07:39</td>\n",
       "      <td>220 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>259356</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>09/19/19 17:30</td>\n",
       "      <td>511 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>259357</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>09/30/19 00:18</td>\n",
       "      <td>250 Meadow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185687 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0        176558        USB-C Charging Cable                2      11.95   \n",
       "2        176559  Bose SoundSport Headphones                1      99.99   \n",
       "3        176560                Google Phone                1        600   \n",
       "4        176560            Wired Headphones                1      11.99   \n",
       "5        176561            Wired Headphones                1      11.99   \n",
       "...         ...                         ...              ...        ...   \n",
       "186845   259353      AAA Batteries (4-pack)                3       2.99   \n",
       "186846   259354                      iPhone                1        700   \n",
       "186847   259355                      iPhone                1        700   \n",
       "186848   259356      34in Ultrawide Monitor                1     379.99   \n",
       "186849   259357        USB-C Charging Cable                1      11.95   \n",
       "\n",
       "            Order Date                         Purchase Address  \n",
       "0       04/19/19 08:46             917 1st St, Dallas, TX 75001  \n",
       "2       04/07/19 22:30        682 Chestnut St, Boston, MA 02215  \n",
       "3       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "4       04/12/19 14:38     669 Spruce St, Los Angeles, CA 90001  \n",
       "5       04/30/19 09:27        333 8th St, Los Angeles, CA 90001  \n",
       "...                ...                                      ...  \n",
       "186845  09/17/19 20:56   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  09/01/19 16:00  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847  09/23/19 07:39     220 12th St, San Francisco, CA 94016  \n",
       "186848  09/19/19 17:30   511 Forest St, San Francisco, CA 94016  \n",
       "186849  09/30/19 00:18   250 Meadow St, San Francisco, CA 94016  \n",
       "\n",
       "[185687 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Display the row 519\n",
    "display(df_no_null_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Type Conversion for Analysis\n",
    "\n",
    "Pandas automatically assigns data types when loading data, but mixed content often results in all columns being classified as 'object' type - the most general but least useful for analysis. Sales data requires proper numeric types for calculations (revenue, quantities) and datetime types for temporal analysis (trends, seasonality). Converting to appropriate data types enables mathematical operations and unlocks analytical capabilities.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.to_numeric()](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html)\n",
    "- [pandas.to_datetime()](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)\n",
    "- [Data type conversion guide](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes)\n",
    "- [Error handling in type conversion](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html#pandas.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Convert columns to appropriate data types for analysis\n",
    "\n",
    "Proper data types are essential for sales analysis. Numeric columns enable revenue calculations and statistical analysis, while datetime columns allow temporal pattern identification. The `errors='coerce'` parameter handles invalid data by converting problematic values to `NaN`, revealing data quality issues while preserving valid records.\n",
    "\n",
    "Convert the following columns to their appropriate types using pandas conversion functions: transform `Order ID`, `Quantity Ordered`, and `Price Each` to numeric types with invalid entries handling (see above). Convert `Order Date` to datetime type using the same error handling. After conversion, verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Convert text to numeric values\n",
    "# Convert Order date to datetime type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check that the conversion has been applied correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check again the missign values. We will notice that one row contains now missing values 'NaN', it is the row 519! One of the ways to isolate the rows with missing values, is to put them in a new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check for missing values again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways to isolate the rows with missing values, is to put them in a new dataframe. Let us then drop rows with missing values and check that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create and display a new DataFrame with rows that have at least one NaN value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Remove rows with NaN values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check if missing rows have been removed again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extracting and Creating Variables\n",
    "\n",
    "In our Sales dataset, we may need to perform analysis and statistics related to a specific month of the year, to a specific city or state, to a specifc category of purchased products. \n",
    "However, the current dataset with current columns does not allow to perform such analysis easily. \n",
    "For this reason, usually, one of the pre-processing steps of a dataset is to create new columns based on existing ones. New colums can be used then directly.\n",
    "Let us make a focus on the columns meaning and content then create some new colums!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Temporal Feature Engineering\n",
    "\n",
    "Sales data exhibits strong temporal patterns that drive business insights: seasonal trends affect product demand, monthly cycles reflect payroll patterns, and daily variations show shopping behaviors. Extracting temporal components from datetime data transforms timestamps into analytical features that reveal these hidden patterns and enable time-based analysis such as trend identification, seasonal forecasting, and peak period optimization.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.Series.dt accessor](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.year.html)\n",
    "- [Time series functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html)\n",
    "- [Datetime properties extraction](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract temporal features for time-based analysis\n",
    "\n",
    "Temporal feature extraction transforms datetime stamps into analytical components that reveal business patterns. Year enables long-term trend analysis, month identifies seasonal variations, day captures monthly cycles, and time reveals daily shopping patterns. These features become essential variables for understanding customer behavior and optimizing business operations.\n",
    "\n",
    "Extract temporal components from the `Order Date` column using pandas datetime accessor methods. Create new columns for `year`, `month`, `day`, and `time` using `.dt.year`, `.dt.month`, `.dt.day`, and `.dt.time` respectively. These extracted features will enable temporal analysis such as identifying peak shopping months, understanding daily sales patterns, and tracking year-over-year growth trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create New Column year,month,day,time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display your dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Numerical Data Transformation\n",
    "\n",
    "Price data often contains decimal precision that may exceed analytical needs. Converting to integer format simplifies calculations, reduces memory usage, and can improve performance for large datasets. However, this transformation should be applied thoughtfully as it permanently removes decimal precision and may not be appropriate for all financial analyses where exact amounts matter.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.DataFrame.astype()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html)\n",
    "- [Data type conversion methods](https://pandas.pydata.org/docs/user_guide/basics.html#basics-astype)\n",
    "- [Numeric data handling](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Convert price data to integer format\n",
    "\n",
    "Integer conversion of price data creates simplified numerical values suitable for certain types of analysis where decimal precision is not critical. This transformation can be useful for categorical price analysis, simplified visualizations, or when working with systems that require whole numbers. The `.astype('int')` method performs this conversion by truncating decimal values.\n",
    "\n",
    "Create a new column called `Price_integer` that contains the price values converted to integer format. This transformation will remove decimal places and create whole number representations of the original prices for simplified analysis scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Convert the 'Price Each' column to integer type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Geographic Data Extraction\n",
    "\n",
    "Geographic analysis of sales data requires extracting location identifiers from address strings. ZIP codes enable regional sales analysis, delivery optimization, and demographic targeting. In this dataset, ZIP codes are consistently formatted as the last 5 characters of the address string, making string slicing an effective extraction method for geographic segmentation and regional performance analysis.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Working with text data in pandas](https://pandas.pydata.org/docs/user_guide/text.html)\n",
    "- [String accessor methods](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html)\n",
    "- [String slicing operations](https://pandas.pydata.org/docs/user_guide/text.html#extracting-substrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Extract ZIP codes for geographic analysis\n",
    "\n",
    "ZIP code extraction enables geographic sales analysis by providing standardized location identifiers. These codes allow grouping transactions by region, analyzing delivery patterns, and identifying high-performing geographic markets. String slicing with negative indexing efficiently extracts the last 5 characters regardless of address length variations.\n",
    "\n",
    "Create a new column called `Pincodes` that contains the ZIP codes extracted from the `Purchase Address` column. Use string slicing with to extract the last 5 characters from each address string, which represent the ZIP codes in this dataset's address format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a new column and store the PIN Codes from the Address column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display your dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Address Parsing for City Extraction\n",
    "\n",
    "City-level analysis is crucial for understanding regional sales performance, distribution logistics, and market penetration. Address strings typically follow standardized formats with comma-separated components. By parsing these structured addresses, we can extract city names for geographic analysis, enabling insights into urban vs suburban performance, regional preferences, and market concentration patterns.\n",
    "\n",
    "**Documentation references:**\n",
    "- [String split operations](https://pandas.pydata.org/docs/user_guide/text.html#splitting-and-replacing-strings)\n",
    "- [pandas.Series.str.split()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html)\n",
    "- [Text data manipulation guide](https://pandas.pydata.org/docs/user_guide/text.html)\n",
    "- [DataFrame concatenation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Parse addresses to extract city information\n",
    "\n",
    "Address parsing transforms unstructured text into structured geographic data for analysis. The comma-delimited format allows systematic extraction of address components, with city names typically appearing as the second element after street information. This extraction enables city-level sales analysis and regional performance comparisons.\n",
    "\n",
    "Parse the `Purchase Address` column by splitting on comma separators to create separate columns for address components. Create a temporary DataFrame to hold the split results, rename the appropriate column to 'City', then concatenate this city information back to the main DataFrame to add the city column alongside existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display the Purchase Address column from the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Split the Purchase Address column into Street, State, and Zip Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Rename the State column to City\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Concatenate the original dataframe with the new dataframe containing City information (concatenate along columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Product Categorization Strategy\n",
    "\n",
    "Large transaction datasets often contain repetitive product purchases that benefit from categorical grouping. Instead of analyzing hundreds of individual product SKUs, grouping similar products into categories (Phones, Accessories, Monitors, Laptops) enables higher-level business insights such as category performance, cross-selling opportunities, and inventory optimization. Text pattern matching provides an efficient method for automated categorization based on product naming conventions.\n",
    "\n",
    "**Documentation references:**\n",
    "- [pandas.unique()](https://pandas.pydata.org/docs/reference/api/pandas.unique.html)\n",
    "- [Regular expressions in Python](https://docs.python.org/3/library/re.html)\n",
    "- [String pattern matching](https://pandas.pydata.org/docs/user_guide/text.html#testing-for-strings-that-match-or-contain-a-pattern)\n",
    "- [Apply function usage](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Analyze product diversity and implement categorization\n",
    "\n",
    "Understanding product variety helps determine the appropriate level of analysis granularity. A dataset with thousands of unique products benefits from categorical grouping, while limited product variety might be analyzed at the individual item level. Pattern-based categorization automates the grouping process using regular expressions to identify product types from naming conventions.\n",
    "\n",
    "First, examine the product diversity to display distinct items and understand if categorization could be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Get all distinct values of products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement pattern-based product categorization system\n",
    "\n",
    "Automate categorization using regular expressions to enable systematic grouping while handling case variations and partial matches efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a limited number of unique products that appear frequently in transactions. Upon examining these products, clear categorization patterns emerge based on their names:\n",
    "\n",
    "- Products containing the word \"phone\" can be categorized as **Phones**\n",
    "- Items labeled with \"Laptop\" belong to the **Laptop** category\n",
    "- Products with \"Monitor\" in their name fall under the **Monitors** category\n",
    "- Items containing terms like \"Cable\", \"Headphones\", or \"Batteries\" can be grouped as **Accessories**\n",
    "\n",
    "This natural categorization allows for more effective analysis of purchasing patterns across product types rather than individual items.\n",
    "\n",
    "First, define a patterns dictionary that maps category names to regular expression patterns for keyword matching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a dictionary with product categories and their corresponding regex patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `categorize_product()` function that uses `re.search()` with case-insensitive matching to identify product categories, returning 'Unknown' for null values and 'Other' for unmatched products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Import re package for dealing efficiently with regular expressions\n",
    "# Create a categorize_product function to categorize products based on patterns\n",
    "def categorize_product(product):\n",
    "    \"\"\" Categorize products based on predefined patterns.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        product : str\n",
    "            The product name to categorize.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The category of the product or 'Unknown' if not found.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this function to the entire `Product` column using `.apply()` to create a new `Category` column, then verify the categorization results by examining the unique categories created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a new column 'Category' based on 'Product' column (use apply method and the categorize_product function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the new categories, the unique ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display the unique categories in the 'Category' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display all rows where the Category is 'Other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Display the first 10 rows of the dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "# 3. Simple descriptive statistical analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply basic statistical methods to summarize and understand data patterns. Students will learn to calculate central tendencies, dispersions, and simple aggregations to extract meaningful insights.\n",
    "\n",
    "Some analysis require grouping by some values (same as in SQL). Learn more about  [grouping](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) operation.\n",
    "\n",
    "**Exercise**: What was the best month for sales? How much was earned that month? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Group by month and sum the Price_integer column to find the best month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Sort the best_month DataFrame by Price_integer in descending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "# 4. Simple visualization\n",
    "\n",
    "Exploratory data analysis relies heavily on effective visualization to reveal hidden patterns, trends, and relationships within datasets. While statistical summaries provide numerical insights, visual representations enable intuitive understanding of data distributions, temporal patterns, and categorical relationships. However, creating effective data visualizations requires more than just plotting data points—it demands thoughtful design choices that follow established principles for clear communication and accurate interpretation.\n",
    "\n",
    "**Documentation references:**\n",
    "- [Seaborn documentation](https://seaborn.pydata.org) - High-level interface for statistical data visualization\n",
    "- [Matplotlib documentation](https://matplotlib.org) - Comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "- [Seaborn tutorial gallery](https://seaborn.pydata.org/examples/index.html) - Examples of different plot types and styling options\n",
    "- [Matplotlib tutorials](https://matplotlib.org/stable/tutorials/index.html) - Comprehensive guides for visualization techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Four Pillars of Quality Data Visualization\n",
    "\n",
    "Based on \"Storytelling with Data\" by Cole Nussbaumer Knaflic (can be borrowed from the school library), every effective visualization must address four fundamental principles:\n",
    "\n",
    "### 1. **Choose the Appropriate Visual**\n",
    "The type of chart you select should match the nature of your data and your analytical goal. For sales data:\n",
    "\n",
    "- **Categorical comparisons** → Use horizontal bar plots for comparing performance across categories\n",
    "  - *Example:* Revenue by product category (Phones vs Laptops vs Accessories)\n",
    "  - *Why horizontal bar plots?* Category names are often long and read better horizontally, with clear visual comparison of discrete categories\n",
    "\n",
    "- **Time series trends** → Use line plots for continuous temporal patterns\n",
    "  - *Example:* Monthly sales evolution over the year\n",
    "  - *Why line plots?* Show progression and trends over time, connecting related time periods\n",
    "\n",
    "- **Temporal distribution analysis** → Use histograms for understanding time-based frequency patterns\n",
    "  - *Example:* Distribution of orders by hour of day or day of week\n",
    "  - *Why histograms?* Reveal peak shopping hours, customer behavior patterns, and temporal concentration\n",
    "\n",
    "- **Relationship exploration** → Use scatter plots for examining correlations\n",
    "  - *Example:* Price vs Quantity relationship to identify bulk purchase patterns\n",
    "  - *Why scatter plots?* Each transaction is a discrete point, revealing patterns without implying false continuity\n",
    "\n",
    "### 2. **Eliminate Clutter** \n",
    "Remove any visual element that doesn't add informational value:\n",
    "- **Grid lines** → Visual noise that distracts from data\n",
    "- **Unnecessary borders** → Chart boxes that don't enhance understanding  \n",
    "- **Redundant elements** → Anything that creates cognitive load without benefit\n",
    "\n",
    "### 3. **Focus Attention**\n",
    "Use visual hierarchy to guide your audience's eye to what matters most:\n",
    "- **Color strategy** → Use accessible, colorblind-friendly palettes\n",
    "- **Emphasis techniques** → Highlight important elements while de-emphasizing context\n",
    "- **Contrast** → Create clear distinction between signal and background\n",
    "\n",
    "### 4. **Ground Principle (Gestalt)**\n",
    "Ensure your visualization has proper foundations for interpretation:\n",
    "- **Clear titles** → Descriptive and informative\n",
    "- **Axis labels with units** → Never leave audience guessing what they're looking at\n",
    "- **Consistent typography** → Professional hierarchy and readability\n",
    "- **Contextual references** → Health thresholds, benchmarks, or comparison points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Configuration Strategy\n",
    "\n",
    "To implement these principles consistently, we establish global styling parameters that automatically apply good design practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global seaborn style - clean background without grid\n",
    "sns.set_style(\"white\", {\n",
    "    \"axes.grid\": False,           # Remove grid lines\n",
    "    \"axes.spines.left\": True,     # Keep left spine\n",
    "    \"axes.spines.bottom\": True,   # Keep bottom spine\n",
    "    \"axes.spines.top\": False,     # Remove top spine\n",
    "    \"axes.spines.right\": False,   # Remove right spine\n",
    "    \"axes.linewidth\": 1.2,        # Slightly thicker axis lines\n",
    "})\n",
    "\n",
    "# Set inclusive and colorblind-friendly palette\n",
    "# Using 'colorblind' palette which is accessible to colorblind users\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why these choices?**\n",
    "- **\"white\" background** → Maximum contrast for data visibility\n",
    "- **Selective spines** → Keep only essential reference lines (left & bottom)\n",
    "- **No grid by default** → Reduces visual noise, lets data stand out\n",
    "- **Colorblind palette** → Ensures accessibility for ~8% of population with color vision deficiency\n",
    "- **Thicker axis lines** → Subtle emphasis on data boundaries without overwhelming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize monthly sales performance\n",
    "\n",
    "Understanding seasonal sales patterns helps identify peak business periods and optimize marketing strategies. Monthly sales analysis reveals customer behavior patterns, seasonal trends, and business cycle variations that inform strategic decisions. Clear visualization with proper month labeling ensures easy interpretation of temporal business patterns.\n",
    "\n",
    "Create a bar plot to visualize the monthly sales performance data you calculated earlier. Use appropriate figure sizing for readability, clear axis labeling, and convert numeric months to readable three-letter month abbreviations (Jan, Feb, Mar, etc.) for professional presentation. Add a descriptive title that clearly communicates what the visualization shows.\n",
    "\n",
    "\n",
    "**Documentation references:**\n",
    "- [sns.barplot()](https://seaborn.pydata.org/generated/seaborn.barplot.html) - Create bar plots for categorical data\n",
    "- [plt.figure()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html) - Set figure size and properties\n",
    "- [plt.xticks()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xticks.html) - Customize x-axis tick labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let us show visually the results of each month for sales. We can visually notice the best one!\n",
    "Let us use a <a href=\"\n",
    "https://seaborn.pydata.org/generated/seaborn.barplot.html\" target=\"_blank\"> matplotlib</a> barplot to do that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Set figure size and plot the monthly sales performance\n",
    "# Plotting the monthly sales performance\n",
    "# Using seaborn's barplot for better aesthetics\n",
    "# Set the title for the plot\n",
    "# Set labels for x and y axes\n",
    "# Convert numeric months to readable month names\n",
    "# Save with high quality settings\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: In which city, we purchase the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Count the number of products sold in each city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: We need to launch an advertising campaign and target the hours with the most purchases.\n",
    "\n",
    "What is the distribution of purchases during the different hours of the day? Which hour/s we should target? Propose a visualisation with seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create a new column that contain hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Group by Order_hour and count the number of orders for each hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the distribution of orders by hour\n",
    "# Select a visualization adapted for hourly data\n",
    "# Set labels and title\n",
    "# Save with high quality settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which product was sold the least as per your categories and what might be the reason behind that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Count the number of products sold in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create seasonal categorization function\n",
    "\n",
    "Sales data often exhibits seasonal patterns that affect product demand and customer behavior. Creating a function to categorize months into seasons enables analysis of seasonal trends, helping identify peak periods for different product categories and optimize inventory management strategies.\n",
    "\n",
    "Create a function called `season()` that takes a numeric month (1-12) as input and returns the corresponding season name as a string. Define four seasons: Spring (March-May), Summer (June-August), Rainy (September-November), and Winter (December-February). Include a comprehensive docstring with parameters, return values, and usage examples following Python documentation standards.\n",
    "\n",
    "```python\n",
    "def season(month):\n",
    "    \"\"\"\n",
    "    Convert a numeric month to its corresponding season.\n",
    "    \n",
    "    This function categorizes months into four seasons based on typical \n",
    "    meteorological patterns, useful for seasonal sales analysis and trend identification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    month : int\n",
    "        Numeric month value (1-12) where 1=January, 2=February, etc.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Season name corresponding to the input month:\n",
    "        - 'Spring': March, April, May (months 3-5)\n",
    "        - 'Summer': June, July, August (months 6-8) \n",
    "        - 'Rainy': September, October, November (months 9-11)\n",
    "        - 'Winter': December, January, February (months 12, 1, 2)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement season function\n",
    "def season(month):\n",
    "    \"\"\"\n",
    "    Convert a numeric month to its corresponding season.\n",
    "    \n",
    "    This function categorizes months into four seasons based on typical \n",
    "    meteorological patterns, useful for seasonal sales analysis and trend identification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    month : int\n",
    "        Numeric month value (1-12) where 1=January, 2=February, etc.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Season name corresponding to the input month:\n",
    "        - 'Spring': March, April, May (months 3-5)\n",
    "        - 'Summer': June, July, August (months 6-8) \n",
    "        - 'Rainy': September, October, November (months 9-11)\n",
    "        - 'Winter': December, January, February (months 12, 1, 2)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which phone is sold most during the Summer season?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Apply the season function to the 'month' column and create a new column 'Season'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which phone is sold most during the month of March?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Select the rows for the month of March and filter for Phones category to find the most sold product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which headphones are the most expensive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Select the rows for the Phones category and sort by price to find the most expensive phone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which product in every category in more likely to be ordered in bulk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Group by category to find the most sold product in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5. Bivariate Statistics: Exploring Correlations and Relationships\n",
    "\n",
    "Bivariate analysis examines relationships between two variables to understand how different factors influence each other in business contexts. Sales data contains interconnected variables where correlation analysis reveals insights into customer behavior, pricing strategies, and market dynamics. This section focuses on visualizing these relationships to identify patterns that drive business performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Continuous Variable Correlation Analysis\n",
    "\n",
    "Correlation analysis between continuous variables reveals how two numerical factors influence each other in sales data. Understanding the relationship between price and quantity helps identify customer purchasing behavior patterns, pricing sensitivity, and bulk purchase trends. Regression plots provide visual correlation assessment while showing the strength and direction of linear relationships.\n",
    "\n",
    "**Documentation references:**\n",
    "- [sns.regplot()](https://seaborn.pydata.org/generated/seaborn.regplot.html) - Create regression plots with correlation visualization\n",
    "- [Correlation analysis guide](https://seaborn.pydata.org/tutorial/regression.html) - Understanding relationships between variables\n",
    "- [Statistical relationship visualization](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize price-quantity correlation patterns\n",
    "\n",
    "Customer purchasing behavior often shows correlations between product price and order quantity. Understanding this relationship reveals pricing sensitivity, bulk purchase patterns, and customer value perception. Regression plots effectively visualize these correlations while showing the strength of linear relationships between continuous variables.\n",
    "\n",
    "Create a regression plot to examine the correlation between product price and quantity ordered. Use `sns.regplot()` with `Price_integer` on the x-axis and `Quantity Ordered` on the y-axis to visualize how price influences purchase quantity. The regression line will show the correlation direction and strength, revealing whether customers tend to order more or fewer items at different price points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Set figure size\n",
    "# Plotting the correlation between Price and Quantity Ordered\n",
    "# Set title and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create basic correlation matrix heatmap\n",
    "\n",
    "Correlation matrices provide comprehensive visualization of relationships between multiple numerical variables simultaneously. Heatmaps represent correlation strength through color intensity, making it easy to identify strong positive and negative relationships at a glance.\n",
    "\n",
    "Create a correlation heatmap for price and quantity variables that displays correlation coefficients with professional styling. Calculate the correlation matrix using `.corr()` on the selected variables, then use `sns.heatmap()` with annotations to show the numerical correlation values. Apply proper color mapping and formatting for clear interpretation.\n",
    "\n",
    "**Documentation references:**\n",
    "- [sns.heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html) - Create correlation heatmaps with customizable styling\n",
    "- [pandas.DataFrame.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) - Calculate correlation matrices between numerical columns\n",
    "- [Seaborn color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html) - Understanding color mapping for correlation visualization\n",
    "\n",
    "Which correlation measure is used by default by `.corr()`? Is this choice relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Prepare the correlation subset\n",
    "# Create the heatmap using diverging color palette RdBu_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Enhance correlation heatmap with statistical significance\n",
    "\n",
    "Understanding the implementation of statistical significance calculation provides insights into correlation robustness. Building a custom correlation function with integrated p-value analysis demonstrates the mathematical foundation behind correlation analysis and enables deeper understanding of statistical validity through seamless visualization integration.\n",
    "\n",
    "**Documentation references:**\n",
    "- [scipy.stats.pearsonr()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) - Calculate Pearson correlation coefficient and p-value\n",
    "- [seaborn.heatmap() annot parameter](https://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn.heatmap) - Custom annotation matrices for enhanced visualization\n",
    "- [Statistical significance interpretation](https://docs.scipy.org/doc/scipy/tutorial/stats.html#correlation-functions) - Understanding p-values in correlation analysis\n",
    "\n",
    "*Step 1: Create the integrated correlation-significance function*\n",
    "\n",
    "Statistical significance testing requires computing both correlation coefficients and p-values for each variable pair to determine if observed correlations could occur by random chance. The function creates a formatted matrix that combines correlation values with significance indicators, preparing data specifically for seaborn heatmap visualization.\n",
    "\n",
    "Understanding heatmap annotation structure is crucial: while the main heatmap displays numeric correlation values for color mapping, the `annot` parameter accepts a separate matrix for text display. This separation allows us to show color-coded correlation strength while annotating with formatted strings that include significance stars.\n",
    "\n",
    "Implement a function that creates a DataFrame of formatted strings matching the dimensions of your correlation matrix. Use nested loops to iterate through all variable pairs, applying `scipy.stats.pearsonr()` to calculate both correlation and significance, then format as strings with appropriate star notation for immediate interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement corr_with_significance function to create formatted correlation matrix\n",
    "def corr_with_significance(df):\n",
    "    \"\"\"\n",
    "    Calculate correlation matrix with integrated statistical significance notation.\n",
    "    \n",
    "    This function computes both correlation coefficients and p-values for all \n",
    "    variable pairs, returning a formatted matrix suitable for direct use in \n",
    "    seaborn heatmap annotations. Each cell contains correlation value followed \n",
    "    by significance stars, enabling immediate visual assessment of relationship \n",
    "    strength and statistical validity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing numerical variables for correlation analysis.\n",
    "        All columns should be numeric types suitable for correlation calculation.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Square matrix of formatted strings with same index/columns as input.\n",
    "        Each cell contains correlation coefficient followed by significance stars:\n",
    "        - \"0.845***\" indicates r=0.845 with p<0.001 (highly significant)\n",
    "        - \"0.234*\" indicates r=0.234 with p<0.05 (significant)\n",
    "        - \"-0.123\" indicates r=-0.123 with p>=0.05 (not significant)\n",
    "        \n",
    "    Matrix Structure:\n",
    "    ----------------\n",
    "    The returned DataFrame has dtype 'object' containing strings, not floats.\n",
    "    This structure is specifically designed for seaborn heatmap annotation:\n",
    "    - Main matrix: df.corr() provides numeric values for color mapping\n",
    "    - Annotation matrix: corr_with_significance() provides text for display\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'A': [1,2,3,4], 'B': [2,4,6,8], 'C': [1,3,2,4]})\n",
    "    >>> result = corr_with_significance(df)\n",
    "    >>> print(result.iloc[0,1])  # \"1.000***\" (perfect positive correlation)\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Diagonal elements show \"1.000\" (perfect self-correlation)\n",
    "    - Uses standard academic notation: *** p<0.001, ** p<0.01, * p<0.05\n",
    "    - Returns strings, not numeric values, for direct heatmap annotation\n",
    "    - Matrix dimensions match df.corr() for proper heatmap alignment\n",
    "    \"\"\"\n",
    "    # Get column names and create result matrix\n",
    "    # Initialize result DataFrame with object dtype for strings\n",
    "    # Calculate correlation and significance for each pair\n",
    "                # Diagonal elements: perfect self-correlation\n",
    "                # Calculate Pearson correlation and p-value\n",
    "                # Determine significance level using standard thresholds\n",
    "                # Format as string with correlation coefficient and significance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Enhance your existing visualization with dual-matrix approach*\n",
    "\n",
    "Now integrate the statistical significance information into your correlation heatmap using seaborn's dual-matrix capability. This approach maintains your existing color scheme based on correlation strength while overlaying significance information as text annotations.\n",
    "\n",
    "The key insight is understanding how seaborn heatmap handles the `annot` parameter: when `annot=True`, it displays the numeric values from the main matrix with formatting controlled by `fmt`. However, when `annot` receives a separate matrix (like our formatted strings), it displays those strings directly, requiring `fmt=''` to prevent additional formatting attempts.\n",
    "\n",
    "Replace your current correlation calculation with the enhanced function and modify the heatmap parameters to utilize both the numeric correlation matrix (for colors) and the formatted annotation matrix (for text display). This dual-matrix approach provides comprehensive correlation analysis in a single, scientifically rigorous visualization.\n",
    "\n",
    "**Key Technical Understanding:**\n",
    "- **Color mapping**: Driven by `df_qty_price.corr()` (numeric values)\n",
    "- **Text display**: Driven by `correlation_with_sig` (formatted strings)  \n",
    "- **Matrix alignment**: Both matrices must have identical dimensions and index/column labels\n",
    "- **Format parameter**: `fmt=''` is mandatory when `annot` contains pre-formatted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Replace df_qty_price.corr() with corr_with_significance() and adjust heatmap parameters\n",
    "# Calculate correlation matrix with significance annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Distribution of quantitative data:  boxplots\n",
    "\n",
    "A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range. \n",
    "\n",
    "\n",
    "Boxplots are represented as follows: \n",
    "\n",
    "<img src=\"images/boxplot.png\" width=700 height=700 />\n",
    "\n",
    "**Documentation references**\n",
    " * [Understanding boxplots](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n",
    " * [Boxplot with seaborn](https://seaborn.pydata.org/generated/seaborn.boxplot.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create and interpret box plots for some variables. We are interested in understanding the distribution of prices in our sales dataset, both overall and by product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO First visualization: Overall price distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Second visualization: Price distribution by category\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge1",
   "language": "python",
   "name": "challenge1_uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
